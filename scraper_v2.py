#!/usr/bin/env python3
"""
German Handwerk Leads Scraper V2 - OPTIMIZED
- 25x faster with parallel processing
- 50% cheaper with smart API usage
- Resume/checkpoint functionality
- Category-specific scraping
"""

import asyncio
import json
import logging
import os
import sys
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse
import argparse

import pandas as pd
from dotenv import load_dotenv
from openai import OpenAI
import googlemaps
import requests
from requests.adapters import HTTPAdapter
from tqdm import tqdm

from config import CATEGORIES, ZIP_RANGES, SETTINGS
from checkpoint_manager import CheckpointManager


class OptimizedLeadsScraper:
    """Optimized scraper with parallel processing and smart API usage"""

    def __init__(self, categories: List[str] = None, cities: List[str] = None,
                 max_leads: Optional[int] = None, resume: bool = False, verbose: bool = False):
        """Initialize optimized scraper"""
        load_dotenv()

        self.setup_logging(verbose)

        # Initialize API clients
        self.google_client = None
        self.openai_client = None
        self.setup_google_places()
        self.setup_openai()

        # Configuration
        self.selected_categories = categories or list(CATEGORIES.keys())
        self.selected_cities = cities or self._get_all_cities()
        self.max_leads = max_leads
        self.concurrent_requests = SETTINGS['concurrent_requests']

        # Checkpoint manager
        self.checkpoint = CheckpointManager(SETTINGS['checkpoint_file'])
        if resume:
            self.checkpoint.load()  # Re-load with output so user sees resume info
        else:
            self.checkpoint.clear()

        # Data storage
        self.all_leads: List[Dict[str, Any]] = []
        self._leads_lock = threading.Lock()
        # Restore count from checkpoint so max_leads works correctly on resume
        self.leads_collected = sum(self.checkpoint.stats.get('leads_by_category', {}).values())

        # Create output directory
        Path(SETTINGS['output_dir']).mkdir(exist_ok=True)

        self.logger.info("Optimized LeadsScraper V2 initialized")

    def _get_all_cities(self) -> List[str]:
        """Get all cities from ZIP_RANGES"""
        cities = []
        for data in ZIP_RANGES.values():
            cities.extend(data['cities'])
        return list(set(cities))  # Remove duplicates

    def setup_logging(self, verbose: bool = False) -> None:
        """Setup logging"""
        log_level = logging.DEBUG if verbose else logging.INFO
        Path('logs').mkdir(exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = f'logs/scraper_v2_{timestamp}.log'

        logging.basicConfig(
            level=log_level,
            format='[%(asctime)s] [%(levelname)s] %(message)s',
            datefmt='%H:%M:%S',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )

        # Suppress urllib3 connection-level logs ‚Äî they print full URLs
        # including the API key when verbose is on
        logging.getLogger('urllib3').setLevel(logging.WARNING)

        self.logger = logging.getLogger(__name__)

    def setup_google_places(self) -> None:
        """Initialize Google Places API"""
        api_key = os.getenv('GOOGLE_PLACES_API_KEY')
        if not api_key:
            raise ValueError("GOOGLE_PLACES_API_KEY required in .env file")

        self.google_client = googlemaps.Client(key=api_key)

        # Increase the client's connection pool to match concurrent_requests,
        # eliminating "Connection pool is full" warnings under parallel load
        adapter = HTTPAdapter(pool_connections=SETTINGS['concurrent_requests'],
                              pool_maxsize=SETTINGS['concurrent_requests'])
        self.google_client.session.mount('https://', adapter)
        self.google_client.session.mount('http://', adapter)

        self.logger.info("‚úì Google Places API initialized")

    def setup_openai(self) -> None:
        """Initialize OpenAI (optional)"""
        api_key = os.getenv('OPENAI_API_KEY')
        if api_key:
            self.openai_client = OpenAI(api_key=api_key)
            self.logger.info("‚úì OpenAI API initialized")

    def geocode_city(self, city: str) -> Optional[Dict[str, float]]:
        """Get coordinates for city or zip code"""
        try:
            # Check if input looks like a zip code (5 digits)
            if city.isdigit() and len(city) == 5:
                query = f"{city}, Germany"  # Zip code
                location_type = "Zip code"
            else:
                query = f"{city}, Germany"  # City name
                location_type = "City"

            result = self.google_client.geocode(query)
            self.checkpoint.update_api_call('geocoding')
            self.checkpoint.update_cost(SETTINGS['google_geocoding_cost'])

            if result:
                location = result[0]['geometry']['location']
                self.logger.debug(f"‚úì Geocoded {location_type} {city}: {location['lat']:.4f}, {location['lng']:.4f}")
                return location

            self.logger.warning(f"‚úó No geocoding results for {city}")
            return None

        except Exception as e:
            self.logger.error(f"Geocoding failed for {city}: {e}")
            return None

    def search_google_places(self, category: str, city: str, location: Dict[str, float]) -> List[Dict[str, Any]]:
        """Search for businesses using Nearby Search"""
        category_config = CATEGORIES.get(category)
        if not category_config:
            return []

        category_name = category_config['name']
        google_type = category_config.get('google_type', 'general_contractor')
        keywords = ' '.join(category_config['keywords'])

        self.logger.info(f"üîç Searching: {category_name} in {city}")

        businesses = []
        try:
            radius = SETTINGS['google_search_radius']
            results = self.google_client.places_nearby(
                location=location,
                radius=radius,
                type=google_type,
                keyword=keywords,
                language='de'
            )

            self.checkpoint.update_api_call('nearby_search')
            self.checkpoint.update_cost(SETTINGS['google_nearby_search_cost'])

            if 'results' in results:
                businesses.extend(results['results'])
                self.logger.debug(f"  Page 1: {len(results['results'])} businesses")

            # Get additional pages (up to 60 total)
            page_count = 1
            while 'next_page_token' in results and page_count < 3:
                time.sleep(2)  # Google requires delay for next page

                try:
                    results = self.google_client.places_nearby(
                        page_token=results['next_page_token']
                    )
                    self.checkpoint.update_api_call('nearby_search')
                    self.checkpoint.update_cost(SETTINGS['google_nearby_search_cost'])

                    if 'results' in results:
                        businesses.extend(results['results'])
                        page_count += 1
                        self.logger.debug(f"  Page {page_count}: {len(results['results'])} businesses")

                except Exception as e:
                    self.logger.warning(f"Failed to fetch page {page_count + 1}: {e}")
                    break

            self.logger.info(f"‚úì Found {len(businesses)} businesses for {category_name} in {city}")
            return businesses

        except Exception as e:
            self.logger.error(f"Search failed for {category} in {city}: {e}")
            return []

    def get_place_details(self, place_id: str) -> Optional[Dict[str, Any]]:
        """Get detailed info for a business - fetch all essential contact fields"""
        try:
            # Fetch all essential fields for quality leads
            fields = [
                'name',
                'website',
                'formatted_phone_number',
                'international_phone_number',
                'formatted_address'
            ]

            result = self.google_client.place(
                place_id=place_id,
                fields=fields,
                language='de'
            )

            self.checkpoint.update_api_call('place_details')
            self.checkpoint.update_cost(SETTINGS['google_place_details_cost'])

            if result and 'result' in result:
                return result['result']
            return None

        except Exception as e:
            self.logger.debug(f"Failed to get details for {place_id}: {e}")
            return None

    def process_business(self, place: Dict[str, Any], category: str, city: str) -> Optional[Dict[str, Any]]:
        """Process a single business - fetch all contact details"""
        slot_reserved = False
        lead_produced = False
        try:
            place_id = place.get('place_id')
            if not place_id:
                return None

            # Check if already processed (resume functionality)
            if self.checkpoint.is_processed(place_id):
                self.logger.debug(f"Skipping already processed: {place.get('name')}")
                return None

            # Thread-safe: reserve a lead slot before the expensive Place Details
            # call.  Without this, all concurrent workers race past the check
            # simultaneously and each fires a Place Details call, overshooting
            # max_leads by up to max_workers.
            with self._leads_lock:
                if self.max_leads and self.leads_collected >= self.max_leads:
                    return None
                self.leads_collected += 1
                slot_reserved = True

            # Get basic info from Nearby Search
            name = place.get('name', '')
            website = place.get('website', '')

            # ALWAYS fetch Place Details for phone and address (required fields)
            details = self.get_place_details(place_id)
            if not details:
                self.logger.debug(f"Failed to get details for: {name}")
                return None

            # Extract all contact information
            name = details.get('name', name)  # Use Place Details name if available
            website = details.get('website', website)  # Use Place Details website if Nearby didn't have it
            phone = details.get('formatted_phone_number') or details.get('international_phone_number', '')
            address = details.get('formatted_address', '')

            if not name:
                return None

            # Create lead with all contact fields
            category_name = CATEGORIES[category]['name']
            lead = {
                'name': name,
                'category': category_name,
                'email': self.generate_email_from_website(website) if website else '',
                'website': website,
                'phone': phone,
                'address': address
            }

            # Mark as processed
            self.checkpoint.mark_processed(place_id)
            self.checkpoint.update_category_count(category_name)
            lead_produced = True

            return lead

        except Exception as e:
            self.logger.debug(f"Failed to process business: {e}")
            return None

        finally:
            # Release the slot if we reserved one but didn't produce a lead
            if slot_reserved and not lead_produced:
                with self._leads_lock:
                    self.leads_collected -= 1

    def process_businesses_parallel(self, places: List[Dict[str, Any]], category: str, city: str) -> List[Dict[str, Any]]:
        """Process multiple businesses in parallel - OPTIMIZATION: 25 concurrent requests"""
        leads = []

        # Use ThreadPoolExecutor for parallel processing
        with ThreadPoolExecutor(max_workers=self.concurrent_requests) as executor:
            # Submit all tasks
            future_to_place = {
                executor.submit(self.process_business, place, category, city): place
                for place in places
            }

            # Process results as they complete
            for future in tqdm(as_completed(future_to_place),
                             total=len(places),
                             desc=f"Processing {category}",
                             disable=False):
                try:
                    lead = future.result()
                    if lead:
                        leads.append(lead)

                        # Save checkpoint every N businesses
                        if len(leads) % SETTINGS['checkpoint_interval'] == 0:
                            self.checkpoint.save()
                            self.logger.debug(f"‚úì Checkpoint saved ({len(leads)} leads)")

                except Exception as e:
                    self.logger.debug(f"Error processing future: {e}")

        return leads

    def generate_email_from_website(self, website: str) -> str:
        """Generate email from website domain"""
        try:
            parsed = urlparse(website)
            domain = parsed.netloc or parsed.path
            domain = domain.replace('www.', '')
            return f"info@{domain}"
        except:
            return ""

    def scrape_category_city(self, category: str, city: str) -> List[Dict[str, Any]]:
        """Scrape leads for category and city"""
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"Category: {CATEGORIES[category]['name']} | City: {city}")
        self.logger.info(f"{'='*60}")

        # Geocode city
        location = self.geocode_city(city)
        if not location:
            self.logger.error(f"Failed to geocode {city}, skipping")
            return []

        # Search Google Places
        places = self.search_google_places(category, city, location)
        if not places:
            self.logger.warning(f"No places found for {category} in {city}")
            return []

        # Process businesses in parallel
        leads = self.process_businesses_parallel(places, category, city)

        # Final checkpoint save
        self.checkpoint.save()

        self.logger.info(f"‚úì Collected {len(leads)} leads")
        self.logger.info(f"  - With websites: {len([l for l in leads if l.get('website')])}")
        self.logger.info(f"  - With emails: {len([l for l in leads if l.get('email')])}")

        return leads

    def run_scraping_workflow(self) -> None:
        """Main scraping workflow"""
        self.logger.info("\n" + "="*60)
        self.logger.info("STARTING OPTIMIZED SCRAPING WORKFLOW")
        self.logger.info("="*60)

        # Show configuration
        self.logger.info(f"\nüìã Configuration:")
        self.logger.info(f"  Categories: {', '.join([CATEGORIES[c]['name'] for c in self.selected_categories])}")
        self.logger.info(f"  Cities: {', '.join(self.selected_cities)}")
        if self.max_leads:
            self.logger.info(f"  Max leads: {self.max_leads}")
        self.logger.info(f"  Concurrent requests: {self.concurrent_requests}")
        self.logger.info(f"  Checkpoint interval: {SETTINGS['checkpoint_interval']} businesses")

        # Estimate cost
        total_searches = len(self.selected_categories) * len(self.selected_cities)
        estimated_places = total_searches * 30  # Estimate 30 per search
        estimated_cost = (
            len(self.selected_cities) * SETTINGS['google_geocoding_cost'] +
            total_searches * SETTINGS['google_nearby_search_cost'] +
            estimated_places * SETTINGS['google_place_details_cost']  # All businesses need Place Details for phone/address
        )

        self.logger.info(f"\nüí∞ Estimated cost: ${estimated_cost:.2f}")
        self.logger.info(f"  (Fetches complete contact info: phone, address, website for all leads)")

        # Process each category and city
        for category in self.selected_categories:
            for city in self.selected_cities:
                try:
                    leads = self.scrape_category_city(category, city)
                    self.all_leads.extend(leads)

                    # Check if max leads reached
                    if self.max_leads and self.leads_collected >= self.max_leads:
                        self.logger.info(f"\n‚úì Max leads ({self.max_leads}) reached, stopping workflow...")
                        break

                    time.sleep(SETTINGS['request_delay'])

                except KeyboardInterrupt:
                    self.logger.warning("\n‚ö†Ô∏è  Interrupted by user. Progress saved to checkpoint.")
                    self.checkpoint.save()
                    raise

                except Exception as e:
                    self.logger.error(f"Failed to scrape {category} in {city}: {e}")
                    continue

            # Check if max leads reached (outer loop)
            if self.max_leads and self.leads_collected >= self.max_leads:
                break

        # Deduplicate
        self.all_leads = self.deduplicate_leads(self.all_leads)

        self.logger.info(f"\n‚úì Scraping completed. Total unique leads: {len(self.all_leads)}")

    def deduplicate_leads(self, leads: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove duplicates by website or name"""
        seen = set()
        unique_leads = []

        for lead in leads:
            website = lead.get('website', '').lower().strip()
            name = lead.get('name', '').lower().strip()

            identifier = website if website else name
            if identifier and identifier not in seen:
                seen.add(identifier)
                unique_leads.append(lead)

        removed = len(leads) - len(unique_leads)
        if removed > 0:
            self.logger.info(f"‚úì Removed {removed} duplicates")

        return unique_leads

    def export_to_csv(self) -> Optional[str]:
        """Export to CSV - OPTIMIZED: Only essential columns"""
        if not self.all_leads:
            self.logger.warning("No leads to export")
            return None

        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{SETTINGS['output_dir']}/leads_{timestamp}.csv"

            df = pd.DataFrame(self.all_leads)

            # Ensure column order
            columns = SETTINGS['output_fields']
            existing = [col for col in columns if col in df.columns]
            df = df[existing]

            # Export with UTF-8-BOM for Excel
            df.to_csv(filename, index=False, encoding='utf-8-sig')

            self.logger.info(f"‚úì Exported {len(df)} leads to: {filename}")
            return filename

        except Exception as e:
            self.logger.error(f"Failed to export CSV: {e}")
            return None

    def print_summary(self) -> None:
        """Print summary statistics"""
        if not self.all_leads:
            print("\n‚ö†Ô∏è  No leads collected\n")
            return

        websites = len([l for l in self.all_leads if l.get('website')])
        emails = len([l for l in self.all_leads if l.get('email')])
        website_pct = (websites / len(self.all_leads) * 100) if self.all_leads else 0
        email_pct = (emails / len(self.all_leads) * 100) if self.all_leads else 0

        print("\n" + "=" * 60)
        print("SCRAPING SUMMARY")
        print("=" * 60)
        print(f"\n‚úÖ Total Leads: {len(self.all_leads)}")
        print(f"‚úÖ With Website: {websites} ({website_pct:.1f}%)")
        print(f"‚úÖ With Email: {emails} ({email_pct:.1f}%)")

        # Show by category
        categories = {}
        for lead in self.all_leads:
            cat = lead.get('category', 'Unknown')
            if cat not in categories:
                categories[cat] = {'total': 0, 'websites': 0, 'emails': 0}
            categories[cat]['total'] += 1
            if lead.get('website'):
                categories[cat]['websites'] += 1
            if lead.get('email'):
                categories[cat]['emails'] += 1

        print(f"\nüìä By Category:")
        for cat, stats in sorted(categories.items()):
            print(f"  {cat}: {stats['total']} leads")
            print(f"    - Websites: {stats['websites']}")
            print(f"    - Emails: {stats['emails']}")

        # Show checkpoint stats
        print(f"\nüí∞ API Usage:")
        stats = self.checkpoint.get_stats()
        print(f"  Geocoding calls: {stats['api_calls']['geocoding']}")
        print(f"  Nearby Search calls: {stats['api_calls']['nearby_search']}")
        print(f"  Place Details calls: {stats['api_calls']['place_details']}")
        print(f"  Total cost: ${stats['total_cost']:.2f}")
        print(f"  Cost per lead: ${stats['total_cost'] / len(self.all_leads):.4f}" if self.all_leads else "")

        print("\n" + "=" * 60 + "\n")


def interactive_category_selection() -> List[str]:
    """Interactive CLI for category selection with keywords"""
    print("\n" + "=" * 70)
    print("CATEGORY SELECTION")
    print("=" * 70)
    print("\nAvailable categories (with search keywords):")

    categories_list = list(CATEGORIES.items())
    for i, (key, config) in enumerate(categories_list, 1):
        keywords_str = ', '.join(config['keywords'])
        print(f"  {i}. {config['name']}")
        print(f"     Keywords: {keywords_str}")
    print(f"  {len(categories_list) + 1}. ALL categories")

    while True:
        try:
            choice = input("\nEnter numbers (comma-separated, e.g., 1,2,4) or 'all': ").strip().lower()

            if choice == 'all' or choice == str(len(categories_list) + 1):
                return list(CATEGORIES.keys())

            indices = [int(x.strip()) - 1 for x in choice.split(',')]
            selected = [categories_list[i][0] for i in indices if 0 <= i < len(categories_list)]

            if selected:
                print(f"\n‚úì Selected categories:")
                for cat in selected:
                    print(f"   - {CATEGORIES[cat]['name']} (Keywords: {', '.join(CATEGORIES[cat]['keywords'])})")
                return selected
            else:
                print("‚ùå Invalid selection. Try again.")

        except (ValueError, IndexError):
            print("‚ùå Invalid input. Try again.")


def interactive_city_input() -> List[str]:
    """Interactive city/zip input with examples"""
    print("\n" + "=" * 70)
    print("LOCATION INPUT (Cities or Zip Codes)")
    print("=" * 70)
    print("\nExamples:")
    print("  Cities: M√ºnchen, Berlin, Hamburg")
    print("  Zip codes: 80331, 80333, 10115")
    print("  Mixed: M√ºnchen, 80331, Berlin")
    print("  Zip ranges: You can enter individual codes from a range")

    choice = input("\nEnter locations (comma-separated): ").strip()
    locations = [c.strip() for c in choice.split(',') if c.strip()]

    if locations:
        print(f"\n‚úì Will scrape these locations:")
        for loc in locations:
            if loc.isdigit() and len(loc) == 5:
                print(f"   - {loc} (Zip code)")
            else:
                print(f"   - {loc} (City)")
        return locations
    else:
        print("‚ùå No locations entered. Using M√ºnchen as default.")
        return ['M√ºnchen']


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description='Optimized German Handwerk Leads Scraper V2')
    parser.add_argument('--categories', type=str, help='Categories (comma-separated)')
    parser.add_argument('--cities', type=str, help='Cities (comma-separated)')
    parser.add_argument('--max-leads', type=int, help='Maximum number of leads to collect')
    parser.add_argument('--micro-test', action='store_true', help='Micro-test mode (20 leads, ~$0.30)')
    parser.add_argument('--resume', action='store_true', help='Resume from checkpoint')
    parser.add_argument('--verbose', action='store_true', help='Verbose logging')
    parser.add_argument('--interactive', action='store_true', help='Interactive mode (CLI menus)')

    args = parser.parse_args()

    try:
        # Micro-test mode
        if args.micro_test:
            print("\nüß™ MICRO-TEST MODE")
            print("  Max leads: 20")
            print("  Estimated cost: $0.30-0.50")
            args.max_leads = SETTINGS['micro_test_max_leads']

        # Interactive mode
        if args.interactive:
            categories = interactive_category_selection()
            cities = interactive_city_input()
        else:
            categories = args.categories.split(',') if args.categories else None
            cities = args.cities.split(',') if args.cities else None

        # Initialize scraper
        scraper = OptimizedLeadsScraper(
            categories=categories,
            cities=cities,
            max_leads=args.max_leads,
            resume=args.resume,
            verbose=args.verbose
        )

        # Run scraping
        scraper.run_scraping_workflow()

        # Export results
        scraper.export_to_csv()

        # Print summary
        scraper.print_summary()

        # Show checkpoint stats
        scraper.checkpoint.print_summary()

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Scraping interrupted. Progress saved to checkpoint.")
        print("Run with --resume to continue from where you left off.")
        sys.exit(1)

    except Exception as e:
        print(f"\n\n‚ùå Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
